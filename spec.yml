project:
  name: CustomerSupportAgent
  description: >
    Demo AI customer support agent for Telegram, using RAG, vector retrieval,
    OpenAI/vLLM LLM serving, and cloud scalability. Designed for multichannel
    expansion (Instagram, Facebook, etc.), optimized token usage (TOON), 
    and advanced observability (Opik).
  version: 0.2.0
  owner: "Your Name"

environment:
  python_version: 3.11
  dependencies:
    - python-telegram-bot==20.7
    - langchain==0.1.0
    - langchain-openai==0.0.2
    - chromadb==0.4.22
    - openai==1.3.0
    - python-dotenv==1.0.0
    - requests
    - pyyaml
    - pytoon # For TOON
    # Suggested for extended/production:
    - rogue-ai  # for RAG infrastructure (future)
    - vllm     # LLM local/OSS serving (future)
    - opik     # Observability/monitoring (future)

architecture:
  components:
    - name: TelegramBot
      description: >
        Receives and sends user messages via Telegram. Webhook & polling support.
        Easy to extend for multiple channels (Instagram/Facebook APIs).
      type: trigger
      interfaces:
        - webhook: /telegram/webhook
        - (future) webhook: /instagram/webhook
        - (future) webhook: /facebook/webhook
    - name: VectorStore
      description: >
        Embeddings, indexing, and retrieval. Pluggable (ChromaDB, Rogue, etc).
      type: vector_store
      storage: ChromaDB
      options:
        - (future) Rogue as advanced vector DB
        - embedding caching enabled
    - name: RAGEngine
      description: >
        Handles splitting, semantic retrieval, prompt/context prep, and calling LLMs.
        Supports TOON formatting for token optimization.
      type: logic
      dependencies:
        - VectorStore
        - LLM (OpenAI/vLLM/OSS)
    - name: LLMService
      description: >
        Handles inference via OpenAI API and/or vLLM local server (OpenAI compatible).
        Supports streaming, model selection, token counting, multi-backend.        
      type: llm
      options:
        - OpenAI
        - vLLM (local/container)
        - Ollama (optional)
    - name: ConfidenceScorer
      description: >
        Scores the model output based on retrieval scores for approval routing.
      type: utility
    - name: ApprovalSystem
      description: >
        Routes ambiguous/medium-confidence responses to admin for review/approval.
        Admin can approve, edit or reject answers. 
      type: workflow
      interfaces:
        - api: /approval
    - name: Observability
      description: >
        Logs, metrics, full LLM/RAG tracing using Opik for root-cause/debugging.
      type: monitoring
      options:
        - Opik (Comet)
        - CloudWatch
        - MLflow

data:
  knowledge_base:
    source_files:
      - knowledge_base.json
    update_frequency: weekly
    format: 
      - json
      - (future) toon
    versioning: enabled

functional_requirements:
  - Receive user messages (Telegram, later Instagram/Facebook)
  - Retrieve context from vector store
  - Use TOON for context/response (if enabled)
  - Generate LLM answer (OpenAI/vLLM/interchangeable)
  - Score confidence for each answer
  - Auto-send high confidence replies
  - Forward low/medium confidence to ApprovalSystem
  - Allow admin to approve, edit, reject
  - Log, trace, and monitor all interactions
  - Easily extend KnowledgeBase, channels and models

non_functional_requirements:
  - Response time < 3 seconds (goal, for <1000 concurrent users)
  - Horizontally scalable via Docker/Lambda (see diagram)
  - All key secrets in secure store
  - Robust error handling, retries, and fallback
  - CI for each code/infra change

testing:
  unit_tests:
    - TestKnowledgeBaseLoading
    - TestRAGSearch
    - TestConfidenceScoring
    - TestTOONConversion
  integration_tests:
    - TelegramMessageFlow
    - ApprovalWorkflow
    - MultiBackendLLM (OpenAI/vLLM switch)
  performance_tests:
    - LatencyUnder3Seconds
    - Stress1000Users

ci_cd:
  pipeline:
    steps:
      - install_dependencies
      - run_unit_tests
      - run_integration_tests
      - build_docker_image
      - deploy_to_aws_lambda
      - deploy_to_ecr
  monitoring:
    tools:
      - Opik
      - CloudWatch
      - MLflow

documentation:
  README: README.md
  architecture_diagrams: docs/architecture.drawio
  user_manual: docs/UserManual.md

notes:
  - This spec is the source of truth.
  - Every infra/feature PR must update this spec.
  - Experimental components (TOON, Rogue, vLLM, Opik) state: optional/future.
  - Enable multi-channel/serving with modular handlers.
